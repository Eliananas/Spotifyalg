{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules, prep spotipy oauth\n",
    "\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyOAuth\n",
    "import cred\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "from scipy.spatial import distance as dist\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from pyclustering.cluster.kmeans import kmeans as pykmeans\n",
    "from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n",
    "from pyclustering.utils.metric import distance_metric, type_metric\n",
    "\n",
    "# Required scopes to access all data needed\n",
    "scope = \"playlist-read-private, playlist-modify-public, user-read-private, user-top-read, user-library-read\"\n",
    "auth_manager = SpotifyOAuth(client_id=cred.client_id, client_secret=cred.client_secret, redirect_uri='http://127.0.0.1:8080', scope=scope)\n",
    "sp = spotipy.Spotify(auth_manager=auth_manager, requests_timeout=10, retries=5)\n",
    "\n",
    "user_id = sp.current_user()['id'] # Ensures we analyze the right user\n",
    "user_country = sp.current_user()['country'] # Ensures we only recommend songs available in the user's country\n",
    "username = sp.current_user()['display_name']\n",
    "\n",
    "# Determines if a user can listen to explicit tracks\n",
    "if sp.current_user()['explicit_content']['filter_enabled'] or sp.current_user()['explicit_content']['filter_locked']:\n",
    "  user_explicit = False\n",
    "else:\n",
    "  user_explicit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def get_user_top_tracks():\n",
    "  '''\n",
    "  Gets ids for a user's top 30 tracks in the long, medium, and short term.\n",
    "\n",
    "  Returns a list of max length 90 with unique top track ids (strings).\n",
    "  '''\n",
    "\n",
    "  long_term_tracks = sp.current_user_top_tracks(time_range='long_term', limit=30)\n",
    "  medium_term_tracks = sp.current_user_top_tracks(time_range='medium_term', limit=30)\n",
    "  short_term_tracks = sp.current_user_top_tracks(time_range='short_term', limit=30)\n",
    "\n",
    "  top_track_ids = []\n",
    "  for ltrack in long_term_tracks['items']:\n",
    "    top_track_ids.append(ltrack['uri'])\n",
    "\n",
    "  for mtrack in medium_term_tracks['items']:\n",
    "    top_track_ids.append(mtrack['uri'])\n",
    "\n",
    "  for strack in short_term_tracks['items']:\n",
    "    top_track_ids.append(strack['uri'])\n",
    "\n",
    "  return list(set(top_track_ids))\n",
    "\n",
    "def get_user_playlist_ids():\n",
    "  '''\n",
    "  Collects a list of user playlist objects and the Spotify id for each of them.\n",
    "\n",
    "  Also returns a list of the original, raw playlist dictionaries as received from the API.\n",
    "  '''\n",
    "  playlists_lst =[]\n",
    "  ids = []\n",
    "  offset = 0\n",
    "  while True:\n",
    "      playlists = sp.current_user_playlists(offset=offset)\n",
    "      if len(playlists['items']) == 0:\n",
    "          break\n",
    "      for playlist in playlists['items']:\n",
    "          playlists_lst.append(playlist)\n",
    "      offset = offset + len(playlists['items'])\n",
    "      time.sleep(0.0001) \n",
    "  \n",
    "  for playlist in playlists_lst:\n",
    "      ids.append(playlist['id'])\n",
    "  return ids, playlists_lst\n",
    "\n",
    "def get_saved_tracks():\n",
    "  '''\n",
    "  Returns a list of the user's \"liked\" tracks (separate from playlist tracks)\n",
    "  '''\n",
    "  ids = []\n",
    "  print('I\\'m starting to look at the user\\'s saved tracks!!')\n",
    "  offset = 0\n",
    "  t1 = time.time()\n",
    "  while True:\n",
    "      track_ids = sp.current_user_saved_tracks(offset=offset)\n",
    "      if len(track_ids['items']) == 0:\n",
    "          break\n",
    "      for track in track_ids['items']:\n",
    "          if track['track'] == None:\n",
    "              continue\n",
    "          else:\n",
    "              ids.append(track['track']['id'])\n",
    "      offset = offset + len(track_ids['items'])\n",
    "      time.sleep(0.0001)\n",
    "  t2 = time.time()\n",
    "  print(f'Hmmm... getting the liked tracks took {t2-t1} seconds!\\n')\n",
    "  return list(set(ids))\n",
    "\n",
    "def get_playlist_names(playlists):\n",
    "  '''\n",
    "  Returns a list of a user's playlist titles when given a list of playlist ids\n",
    "  '''\n",
    "  names = []\n",
    "  for playlist in playlists:\n",
    "      name = sp.playlist(playlist)['name']\n",
    "      names.append(name)\n",
    "  return names\n",
    "\n",
    "def get_song_ids_from_playlists(user, playlist_urls):\n",
    "  '''\n",
    "  Gets song ids from each of the songs in given playlist ids\n",
    "\n",
    "  Returns list of unique song ids (contains no duplicates)\n",
    "  '''\n",
    "  ids = []\n",
    "  t1 = time.time()\n",
    "  for i in range(len(playlist_urls)):\n",
    "      offset = 0\n",
    "      playlist_name = get_playlist_names([playlist_urls[i]])\n",
    "      print(f'I\\'m grabbing saved songs from playlist number {i+1} out of {len(playlist_urls)}: {playlist_name[0]}')\n",
    "      while True:\n",
    "          track_ids = sp.user_playlist_tracks(user=user, playlist_id=playlist_urls[i], offset=offset, fields ='items.track.id')\n",
    "          #print(track_ids)\n",
    "          #print(len(track_ids['items']))\n",
    "          if len(track_ids['items']) == 0:\n",
    "              break\n",
    "          for track in track_ids['items']:\n",
    "              if track['track'] == None:\n",
    "                  continue\n",
    "              else:\n",
    "                  ids.append(track['track']['id'])\n",
    "          offset = offset + len(track_ids['items'])\n",
    "          time.sleep(0.0001)\n",
    "  t2 = time.time()\n",
    "  print(f'Getting song ids from all those playlists took {round(t2-t1, 2)} seconds!\\n')\n",
    "  return list(set(ids))\n",
    "\n",
    "def get_recc_ids(list_seed_tracks, country):\n",
    "  '''\n",
    "  Gets ids for 30 recommended songs for each song a user's playlists\n",
    "\n",
    "  Returns list of ids for the potential recommended songs. \n",
    "  '''\n",
    "  print('Starting to collect recommendation ids.')\n",
    "  if len(list_seed_tracks) > 150:\n",
    "    print(f'Wow! I have {len(list_seed_tracks)*20} to make. This may take a while.\\n')\n",
    "\n",
    "  recc_ids = []\n",
    "  #raw_recs = []\n",
    "  t1 = time.time()\n",
    "  for seed in list_seed_tracks:\n",
    "    seed_to_use = []\n",
    "    seed_to_use.append(seed)\n",
    "    recs = sp.recommendations(seed_tracks=seed_to_use, limit = 30, country=country)\n",
    "    #raw_recs.append(recs)\n",
    "    #print(recs)\n",
    "    for i in range(len(recs['tracks'])):\n",
    "      track_id = recs['tracks'][i]['id']\n",
    "      if track_id not in recc_ids:\n",
    "        recc_ids.append(track_id)\n",
    "    #print(len(recc_ids))\n",
    "  set_ids = set(recc_ids) # extra check to make sure there are no duplicates\n",
    "  t2 = time.time()\n",
    "  print(f'Making and saving all of those recommendations took {round(t2-t1, 2)} seconds.\\n')\n",
    "  return list(set_ids)\n",
    "\n",
    "def create_playlist(tracks):\n",
    "  '''\n",
    "  tracks: list of song ids to add\n",
    "\n",
    "  Creates a new playlist for the user and adds in the provided tracks.\n",
    "  '''\n",
    "  sp.user_playlist_create(user_id, 'your recommended songs', description='yay new songs!')\n",
    "  user_playlists, y = get_user_playlist_ids()\n",
    "  sp.user_playlist_add_tracks(user_id, user_playlists[0], tracks)\n",
    "  return 'Your playlist has been created!'\n",
    "\n",
    "def create_df(track_ids, in_lib):\n",
    "  '''\n",
    "  Creates a massive dataframe with choosen attributes for each song in track_ids\n",
    "\n",
    "  in_lib: the ids in track_ids come from a user's saved tracks (1) or potential tracks to recommend (0)\n",
    "  '''\n",
    "  print(f'{len(track_ids)} observations to make!')\n",
    "  data = []\n",
    "\n",
    "  for i in range(len(track_ids)):\n",
    "    # Get raw data for track\n",
    "    try:\n",
    "      track = sp.track(track_ids[i])\n",
    "      features = sp.audio_features(track_ids[i])\n",
    "      analysis = sp.audio_analysis(track_ids[i])\n",
    "      artist_uri = track['album']['artists'][0]['uri']\n",
    "      artist = sp.artist(artist_uri)\n",
    "      decade_prep = track['album']['release_date'][0:3]\n",
    "      decade = int(decade_prep + '0')\n",
    "      if int(track['album']['release_date'][3]) >= 0 and int(track['album']['release_date'][3]) < 5:\n",
    "        half_decade = int(track['album']['release_date'][0:3] + '0')\n",
    "      else:\n",
    "        half_decade = int(track['album']['release_date'][0:3] + '5')\n",
    "    except:\n",
    "      print(f'skipped one ({in_lib})!')\n",
    "      continue\n",
    "\n",
    "    \n",
    "    \n",
    "    # Extract relevant data\n",
    "    observation = [\n",
    "      track['uri'], \n",
    "      track['name'],\n",
    "      in_lib,\n",
    "      artist['followers']['total'],\n",
    "      artist['genres'],\n",
    "      artist['popularity'],\n",
    "      track['explicit'],\n",
    "      track['album']['release_date'][0:4], \n",
    "      int(track['album']['release_date'][0:4]),\n",
    "      decade,\n",
    "      half_decade,\n",
    "      len(track['artists']),\n",
    "      track['duration_ms'],\n",
    "      track['popularity'],\n",
    "      features[0]['danceability'],\n",
    "      features[0]['energy'],\n",
    "      features[0]['key'],\n",
    "      analysis['track']['key_confidence'],\n",
    "      features[0]['loudness'],\n",
    "      features[0]['mode'],\n",
    "      analysis['track']['mode_confidence'],\n",
    "      features[0]['speechiness'],\n",
    "      features[0]['acousticness'],\n",
    "      features[0]['instrumentalness'],\n",
    "      features[0]['liveness'],\n",
    "      features[0]['valence'],\n",
    "      features[0]['tempo'],\n",
    "      analysis['track']['tempo_confidence'],\n",
    "      features[0]['time_signature'],\n",
    "      analysis['track']['time_signature_confidence'],\n",
    "      analysis['track']['num_samples'],\n",
    "      len(analysis['bars']),\n",
    "      len(analysis['beats']),\n",
    "      len(analysis['sections']),\n",
    "      len(analysis['segments']), # for each segment, there is a list of pitches and timbre!\n",
    "      len(analysis['tatums'])\n",
    "    ]\n",
    "\n",
    "    # Add observation to total dataset\n",
    "    data.append(observation)\n",
    "    time.sleep(0.00000001)\n",
    "    \n",
    "  # Create final data frame with proper column names\n",
    "  df = pd.DataFrame(data, columns=[\n",
    "    'uri', 'track_name', 'in_library', 'artist_followers', 'genre', 'artist_popularity', 'explicit', 'release_date', \n",
    "    'year', 'decade', 'half_decade', 'nartists', 'duration_ms', 'track_popularity', 'danceability', 'energy',\n",
    "    'key', 'key_conf', 'loudness', 'mode', 'mode_conf', 'speechiness', 'acousticness', 'instrumentalness',\n",
    "    'liveness', 'valence', 'tempo', 'tempo_conf', 'time_sig', 'time_sig_conf', 'nsamples', 'nbars',\n",
    "    'nbeats', 'nsections', 'nsegments', 'ntatums'\n",
    "  ])\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_manage(reccs, saved):\n",
    "  '''\n",
    "  Creates a dataframe for all potential recommendations, all saved tracks, and both of them combined\n",
    "\n",
    "  reccs: list of potential recommendation ids\n",
    "  saved: list of saved track ids\n",
    "  '''\n",
    "  recc_df = create_df(reccs, 0) # df of recommendations\n",
    "  saved_df = create_df(saved, 1) # df of songs in library\n",
    "  combined_df = pd.concat([recc_df, saved_df], ignore_index = True) # the previous two combined\n",
    "  return recc_df, saved_df, combined_df\n",
    "\n",
    "def count_predict(kmeans, user_weights, group, clusters, recc_df, saved_df, num_feat, ord_feat, nom_feat):\n",
    "  '''\n",
    "  Returns array of recommendation cluster predictions, number of recommendations per predicted cluster, number of saved tracks per predicted cluster, percentages of saved songs in each cluster, and the number of tracks to recommend (proportionally by number of saved songs in each cluster)\n",
    "  '''\n",
    "  \n",
    "  scaled_reccs, x = scaling(group, recc_df, user_weights, num_feat, ord_feat, nom_feat)\n",
    "  scaled_saved, y = scaling(group, saved_df, user_weights, num_feat, ord_feat, nom_feat)\n",
    "\n",
    "  # Predict clusters for recommendations\n",
    "  recc_predictions = kmeans.predict(scaled_reccs)\n",
    "  \n",
    "  # Predict clusters for saved tracks\n",
    "  saved_predictions = kmeans.predict(scaled_saved)\n",
    "\n",
    "  # Set counts for all recc clusters to 0 (ensures that all clusters are present)\n",
    "  initial_recc_counts = {}\n",
    "  for i in range(0, clusters):  \n",
    "    initial_recc_counts[i] = 0\n",
    "\n",
    "  # Create a counter and add in predicted cluster counts for recommendations\n",
    "  cluster_recc_counts = Counter(initial_recc_counts)\n",
    "  cluster_recc_counts.update(recc_predictions)\n",
    "\n",
    "  # Set counts for all saved clusters to 0 (ensures that all clusters are present)\n",
    "  initial_saved_counts = {}\n",
    "  for i in range(0, clusters):  \n",
    "    initial_saved_counts[i] = 0\n",
    "\n",
    "  # Create a counter and add in predicted cluster counts for saved tracks\n",
    "  cluster_saved_counts = Counter(initial_saved_counts)\n",
    "  cluster_saved_counts.update(saved_predictions)\n",
    "\n",
    "  # Create new dict with percentages of songs in library that are in each of the clusters\n",
    "  cluster_prop = {}\n",
    "  for item in cluster_saved_counts:\n",
    "    cluster_prop[item] = cluster_saved_counts[item] / saved_df.shape[0]\n",
    "\n",
    "  n_to_recc = {}\n",
    "  for item in cluster_prop:\n",
    "    n_to_recc[item] = round(cluster_prop[item]*30, 7)\n",
    "  \n",
    "  return recc_predictions, cluster_recc_counts, cluster_saved_counts, cluster_prop, n_to_recc\n",
    "\n",
    "def add_cluster(recc_data, saved_data, clusters):\n",
    "  '''\n",
    "  Adds a new column to provided dataframe indicating the cluster in which each track is predicted to belong\n",
    "\n",
    "  Note: all saved tracks are given the value of max(clusters) + 1 for use in diagnostics to easily see where the saved tracks fall\n",
    "\n",
    "  Returns a dataframe with just potential recommendations, their values for all variables, and their predicted clusters and a dataframe with both potential recommendations and saved tracks combined\n",
    "  '''\n",
    "  recc_with_clusters = recc_data.assign(cluster = clusters)\n",
    "  saved_with_clusters = saved_data.assign(cluster = max(clusters) + 1)\n",
    "  new_combined = pd.concat([recc_with_clusters, saved_with_clusters], ignore_index = True)\n",
    "  return recc_with_clusters, new_combined\n",
    "\n",
    "def silhouette_plot(dist_metric, scaled_og_data):\n",
    "  '''\n",
    "  Creates a silhouette plot and prints silhouette score for a specific range of clusters\n",
    "\n",
    "  Used in an attempt to determine the number of clusters\n",
    "  '''\n",
    "  range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "  for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-1, 1]\n",
    "    ax1.set_xlim([-1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(scaled_og_data) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    initial_centers = kmeans_plusplus_initializer(data = scaled_og_data, amount_centers = n_clusters).initialize()\n",
    "    clusterer = pykmeans(scaled_og_data, initial_centers, metric = dist_metric)\n",
    "    clusterer.process()\n",
    "    #clusters = kmeans_instance.get_clusters()\n",
    "    #final_centers = kmeans_instance.get_centers()\n",
    "    # type(final_centers[0])\n",
    "    cluster_labels = clusterer.predict(scaled_og_data)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(scaled_og_data, cluster_labels)\n",
    "    print(\n",
    "        \"For \",\n",
    "        n_clusters,\n",
    "        \"clusters, the average silhouette score is:\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(scaled_og_data, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "      # Aggregate the silhouette scores for samples belonging to\n",
    "      # cluster i, and sort them\n",
    "      ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "      ith_cluster_silhouette_values.sort()\n",
    "\n",
    "      size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "      y_upper = y_lower + size_cluster_i\n",
    "\n",
    "      color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "      ax1.fill_betweenx(\n",
    "        np.arange(y_lower, y_upper),\n",
    "        0,\n",
    "        ith_cluster_silhouette_values,\n",
    "        facecolor=color,\n",
    "        edgecolor=color,\n",
    "        alpha=0.7,\n",
    "      )\n",
    "\n",
    "      # Label the silhouette plots with their cluster numbers at the middle\n",
    "      ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "      # Compute the new y_lower for next plot\n",
    "      y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-1, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "      scaled_og_data[:, 0], scaled_og_data[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = np.array(clusterer.get_centers())\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(\n",
    "      centers[:, 0],\n",
    "      centers[:, 1],\n",
    "      marker=\"o\",\n",
    "      c=\"white\",\n",
    "      alpha=1,\n",
    "      s=200,\n",
    "      edgecolor=\"k\",\n",
    "    )\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "      ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 3rd feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 4th feature\")\n",
    "\n",
    "    plt.suptitle(\n",
    "      \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
    "      % n_clusters,\n",
    "      fontsize=14,\n",
    "      fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "def diagnostics(algo, metric, scaled_data, fit_data, vars, pairplot, silhouette, elbow):\n",
    "  '''\n",
    "  Runs basic diagnostics on a K-Means performance: \n",
    "  \n",
    "  pairplot: if only two variables are supplied, a scatter plot is produced, otherwise a pairplot\n",
    "  silhouette: runs a silhouette analysis, providing silhouette scores and plots\n",
    "  elbow: created an elbow plot, only works with sklearn's K-Means which is currently deprecated\n",
    "  '''\n",
    "  if pairplot:\n",
    "    if len(vars) == 2:\n",
    "      sns.scatterplot(data = fit_data, x = vars[0], y = vars[1])\n",
    "      plt.show()\n",
    "\n",
    "      sns.scatterplot(data = fit_data, x = vars[0], y = vars[1], hue = 'cluster', palette = 'Spectral')\n",
    "      plt.show()\n",
    "    else:\n",
    "      sns.pairplot(data = fit_data, vars = vars, hue = 'cluster', palette = 'Spectral')\n",
    "      plt.show()\n",
    "  \n",
    "  if silhouette:\n",
    "    silhouette_plot(metric, scaled_data)\n",
    "\n",
    "  if elbow and algo == 'sklearn':\n",
    "    distortions = []\n",
    "    for k in range(2, 15):\n",
    "      kmeans = KMeans(n_clusters=k, random_state=10, init='k-means++')\n",
    "      kmeans.fit(scaled_data)\n",
    "      distortions.append(kmeans.inertia_)\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    plt.plot(range(2, 15), distortions)\n",
    "    plt.grid(True)\n",
    "    plt.title('Elbow curve')\n",
    "  \n",
    "def detect_outlier(scaled_data, combined_df):\n",
    "  '''\n",
    "  Uses three outlier detection algorithms to find outliers\n",
    "\n",
    "  Returns list of all outlier indices (unduplicated) from all three methods\n",
    "  '''\n",
    "  \n",
    "  from sklearn.ensemble import IsolationForest\n",
    "  iso = IsolationForest(contamination=0.1)\n",
    "  yhat_iso = iso.fit_predict(scaled_data)\n",
    "  obs_iso = np.where(yhat_iso == -1)[0]\n",
    "\n",
    "  from sklearn.neighbors import LocalOutlierFactor\n",
    "  lof = LocalOutlierFactor()\n",
    "  yhat_lof = lof.fit_predict(scaled_data)\n",
    "  obs_lof = np.where(yhat_lof == -1)[0]\n",
    "\n",
    "  from sklearn.svm import OneClassSVM\n",
    "  ee = OneClassSVM(nu=0.01)\n",
    "  yhat_ee = ee.fit_predict(scaled_data)\n",
    "  obs_ocs = np.where(yhat_ee == -1)[0]\n",
    "\n",
    "  common_outliers = sorted(set(obs_iso).union(set(obs_lof), set(obs_ocs)))\n",
    "  # print(common_outliers)\n",
    "  recc_outliers = []\n",
    "  for index in common_outliers:\n",
    "    if combined_df.iloc[index]['in_library'] == 0:\n",
    "      recc_outliers.append(index)\n",
    "\n",
    "  print(f'Nice! There are {len(recc_outliers)} outliers.')\n",
    "  return recc_outliers\n",
    "\n",
    "def manhattan(object1, object2):\n",
    "  return dist.cityblock(object1, object2)\n",
    "\n",
    "def chebyshev(object1, object2):\n",
    "  return dist.chebyshev(object1, object2)\n",
    "\n",
    "def minkowski6(object1, object2):\n",
    "  if len(object1.shape) > 1 or len(object2.shape) > 1:\n",
    "    return np.power(np.sum(np.power(object1 - object2, 6), axis=1), 1/6)\n",
    "  else:\n",
    "    return np.power(np.sum(np.power(object1 - object2, 6)), 1 / 6)\n",
    "\n",
    "def minkowski12(object1, object2):\n",
    "  if len(object1.shape) > 1 or len(object2.shape) > 1:\n",
    "    return np.power(np.sum(np.power(object1 - object2, 12), axis=1), 1/12)\n",
    "  else:\n",
    "    return np.power(np.sum(np.power(object1 - object2, 12)), 1 / 12)\n",
    "\n",
    "def canberra(object1, object2):\n",
    "  return dist.canberra(object1, object2)\n",
    "\n",
    "def euclidean(object1, object2):\n",
    "  return dist.euclidean(object1, object2)\n",
    "\n",
    "def show_playlists(playlist_ids):\n",
    "  '''\n",
    "  Prints all user playlists, numbered\n",
    "\n",
    "  Used for allowing user's to remove certain playlists from the analysis\n",
    "  '''\n",
    "\n",
    "  numbered_names = []\n",
    "  for i in range(len(playlist_ids)):\n",
    "    numbered_name = str(i + 1) + '.' + ' ' + playlist_names[i]\n",
    "    numbered_names.append(numbered_name)\n",
    "  for name in numbered_names:\n",
    "    print(name)\n",
    "\n",
    "def remove_explicit(recc_ids):\n",
    "  '''\n",
    "  Removes all explicit songs from recommendation ids if a user\n",
    "\n",
    "  Dependent on Spotify data; songs that are explicit but not marked as explicit could still be recommended\n",
    "  '''\n",
    "  for recc in recc_ids:\n",
    "    if sp.track(recc)['explicit']:\n",
    "      recc_ids.remove(recc)\n",
    "\n",
    "def scaling(group, combined_df, weights, numeric_features, ordinal_features, nominal_features):\n",
    "\n",
    "  '''\n",
    "  Standardizes all data depending on user group.\n",
    "  \n",
    "  Returns numpy array of all scaled data and a list of all variables used\n",
    "  '''\n",
    "\n",
    "  num_transformer = StandardScaler()\n",
    "  ordnom_transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"ord\", OrdinalEncoder(), ordinal_features),\n",
    "            ('nom', OneHotEncoder(), nominal_features)\n",
    "        ]\n",
    "    )\n",
    "  \n",
    "  if group in [2, 4]:\n",
    "    scaled_data = num_transformer.fit_transform(combined_df[numeric_features])\n",
    "    all_vars = numeric_features\n",
    "  elif group in [6, 8]:\n",
    "    scaled_data = num_transformer.fit_transform(combined_df[numeric_features])\n",
    "    for i in range(len(weights)):\n",
    "      scaled_data[:, i] *= weights[i]**(1/2)\n",
    "    all_vars = numeric_features\n",
    "  \n",
    "  elif group in [3, 5]:\n",
    "    scaled_num = num_transformer.fit_transform(combined_df[numeric_features])\n",
    "    scaled_ordnom = ordnom_transformer.fit_transform(combined_df[ordinal_features + nominal_features])\n",
    "    scaled_data = np.concatenate((scaled_num, scaled_ordnom), axis = 1)\n",
    "    all_vars = numeric_features + ordinal_features + nominal_features\n",
    "  elif group in [7, 9]:\n",
    "    scaled_num = num_transformer.fit_transform(combined_df[numeric_features])\n",
    "    scaled_ordnom = ordnom_transformer.fit_transform(combined_df[ordinal_features + nominal_features])\n",
    "    for i in range(len(weights)):\n",
    "      scaled_num[:, i] *= weights[i]**(1/2)\n",
    "    scaled_data = np.concatenate((scaled_num, scaled_ordnom), axis = 1)\n",
    "    all_vars = numeric_features + ordinal_features + nominal_features\n",
    "  \n",
    "  return scaled_data, all_vars\n",
    "\n",
    "def kmeans_process(distance, group, user_weights, combined_df, recc_df, saved_df):\n",
    "  '''\n",
    "  Runs the K-Means algorithm and removes outliers\n",
    "\n",
    "  Returns final scaled data (numpy array), predicted clusters for potential recommendations (numpy array), number of tracks to recommend from each cluster, all variables used in clustering, and the distance metric used \n",
    "  '''\n",
    "\n",
    "  # Set number of clusters to be used in K-Means\n",
    "  if distance in ['chebyshev', 'minkowski_6', 'euclidean', 'minkowski_12']:\n",
    "    clusters = 3\n",
    "  else:\n",
    "    clusters = 2\n",
    "\n",
    "  # All potential variables to be analyzed\n",
    "  numeric_features = ['artist_popularity',\n",
    "    'duration_m', 'track_popularity', 'danceability', 'energy',\n",
    "    'loudness', 'acousticness', 'instrumentalness',\n",
    "    'liveness', 'tempo', 'valence']\n",
    "  ordinal_features = ['half_decade', 'key', 'time_sig']\n",
    "  if user_explicit:\n",
    "    nominal_features = ['explicit', 'mode']\n",
    "  else:\n",
    "    nominal_features = ['mode']\n",
    "\n",
    "  # Standarizes data; results depend on user group, distance metric, and possible weights\n",
    "  scaled_data, all_vars = scaling(group, combined_df, user_weights, numeric_features, ordinal_features, nominal_features)\n",
    "  \n",
    "  # Sets distance metric for K-Means\n",
    "  metric = distance_metric(type_metric.USER_DEFINED, func=distance)\n",
    "\n",
    "  # Use K-Means++ for initial guesses on centers\n",
    "  initial_centers = kmeans_plusplus_initializer(data = scaled_data, amount_centers = clusters).initialize()\n",
    "  kmeans = pykmeans(scaled_data, initial_centers, metric = metric)\n",
    "  kmeans.process() # required for pyclustering\n",
    "\n",
    "  # Details for clusters without removing outliers\n",
    "  # recc_clusters, recc_count, saved_count, saved_prop, nto_recc = count_predict(kmeans, user_weights, group, clusters, recc_df, saved_df, all_vars, numeric_features, ordinal_features, nominal_features)\n",
    "  # recc_with_cluster, combined_with_cluster = add_cluster(recc_df, saved_df, recc_clusters)\n",
    "\n",
    "  # Remove outliers from data and scale new data\n",
    "  new_recc_df = recc_df.drop(detect_outlier(scaled_data, combined_df), axis = 0, inplace = False)\n",
    "  new_combined_df = pd.concat([new_recc_df, saved_df], ignore_index = True)\n",
    "  new_scaled_data, _ = scaling(group, new_combined_df, user_weights, numeric_features, ordinal_features, nominal_features)\n",
    "\n",
    "  # Rerun K-Means with newly-scaled data\n",
    "  new_initial_centers = kmeans_plusplus_initializer(data = new_scaled_data, amount_centers = clusters).initialize()\n",
    "  new_kmeans = pykmeans(new_scaled_data, new_initial_centers, metric = metric)\n",
    "  new_kmeans.process() # required for pyclustering\n",
    "\n",
    "  # Find counts per cluster, number of saved tracks per cluster, and number of songs to recommend\n",
    "  # Number to recommend depends on proportion of saved tracks in the cluster. \n",
    "  # If 25% of saved tracks are in cluster 1, 25% of recommendations will also come from cluster 1\n",
    "  new_recc_clusters, new_recc_count, new_saved_count, new_saved_prop, new_nto_recc = count_predict(new_kmeans, user_weights, group, clusters, new_recc_df, saved_df, numeric_features, ordinal_features, nominal_features)\n",
    "\n",
    "  # Creates new dataframe with cluster column (for use with diagnostics)\n",
    "  new_recc_df_clusters, new_ncombined = add_cluster(new_recc_df, saved_df, new_recc_clusters)\n",
    "\n",
    "  return new_scaled_data, new_recc_clusters, new_nto_recc, new_ncombined, all_vars, metric\n",
    "\n",
    "def group1(og_reccs, in_lib_tracks):\n",
    "  '''\n",
    "  Generate 30 random recommendations, not considering K-Means\n",
    "\n",
    "  For Group 1 only!\n",
    "  '''\n",
    "\n",
    "  random.shuffle(og_reccs)\n",
    "\n",
    "  if len(og_reccs) <= 30:\n",
    "    group1 = og_reccs\n",
    "  else:\n",
    "    group1 = []\n",
    "    for i in range(len(og_reccs)):\n",
    "      if og_reccs[i] not in in_lib_tracks:\n",
    "        group1.append(og_reccs[i])\n",
    "      if len(group1) == 30:\n",
    "        break\n",
    "\n",
    "  return group1\n",
    "\n",
    "def other_groups(to_recc_counts, clustered_data, in_lib_tracks):\n",
    "  '''\n",
    "  Choose random songs to recommend. Chosen proportionally to how many saved tracks are in each cluster.\n",
    "\n",
    "  The number of songs to recommend will be around 30, but not exactly due to rounding.\n",
    "  '''\n",
    "  group2 = []\n",
    "  \n",
    "  for cluster in to_recc_counts.keys():\n",
    "    one_cluster = clustered_data[clustered_data.cluster.eq(cluster)] # Select one cluster\n",
    "    uris = list(one_cluster['uri']) # Select only uris\n",
    "    reccs = random.sample(uris, k = round(to_recc_counts[cluster])) # take a random sample of the correct size\n",
    "    for i in range(len(reccs)):\n",
    "      group2.append(reccs[i]) # Append uris to group2\n",
    "\n",
    "  # Remove any tracks already in library\n",
    "  # Might not work yet\n",
    "  for i in range(len(group2)):\n",
    "    if group2[i] in in_lib_tracks:\n",
    "      group1.remove(group2[i])\n",
    "  return group2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all playlist ids and all complete playlist information\n",
    "playlist_ids, raw_playlists = get_user_playlist_ids()\n",
    "print(f'Cool! You have {len(playlist_ids)} playlists in your library!')\n",
    "\n",
    "# Collect playlist names\n",
    "playlist_names = get_playlist_names(playlist_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all playlists, allow user to remove certain playlists from being recommended\n",
    "\n",
    "while True:\n",
    "\n",
    "  print(f'I see {len(playlist_ids)} playlists in your library! But you may not want all of those to be used for recommendations. Type in the number of the playlist you want removed. When all you\\'re finished, type \\\"Done\\\".')\n",
    "\n",
    "  show_playlists(playlist_ids)\n",
    "\n",
    "  list_to_remove = input('Type number here!')\n",
    "  print(list_to_remove)\n",
    "\n",
    "  if list_to_remove == 'Done':\n",
    "    break\n",
    "  else:\n",
    "    del playlist_ids[int(list_to_remove) - 1]\n",
    "    del playlist_names[int(list_to_remove) - 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the song ids for all tracks in the selected playlist(s)\n",
    "list_ids = get_song_ids_from_playlists(user_id, playlist_ids[22:50])\n",
    "liked_ids = get_saved_tracks()\n",
    "saved_ids = list_ids + liked_ids\n",
    "print(f'Whoa...you have {len(saved_ids)} songs in your library (using the playlists provided).')\n",
    "\n",
    "# Get potential recommendation ids\n",
    "recc_ids = get_recc_ids(saved_ids, user_country)\n",
    "\n",
    "# Remove explicit tracks from recommendations if user cannot listen to them\n",
    "if not user_explicit:\n",
    "  remove_explicit(recc_ids)\n",
    "\n",
    "# Get user's most-listened-to tracks\n",
    "top_track_ids = get_user_top_tracks()\n",
    "\n",
    "# Add top tracks to tracks in user's library\n",
    "for top_track in top_track_ids:\n",
    "  if top_track not in saved_ids:\n",
    "    saved_ids.append(top_track)\n",
    "    saved_ids.append(top_track) # Add track twice since these tracks are clearly enjoyed by the user (or perhaps their children? :D)\n",
    "  else:\n",
    "    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build data frames\n",
    "# Takes a *very* long time\n",
    "\n",
    "recc_df, saved_df, combined_df = df_manage(recc_ids, saved_ids)\n",
    "\n",
    "# 8986: ~93 minutes?\n",
    "# 14,278: ~145 minutes\n",
    "# 22,127: ~234 minutes\n",
    "# 43,721: < 550 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 <function euclidean at 0x123c949d0>\n"
     ]
    }
   ],
   "source": [
    "# Scale all data, fit new kmeans\n",
    "\n",
    "# Group 1: Random 30 basic reccs\n",
    "\n",
    "# Group 2: Euclidean distance with only numeric vars\n",
    "# Group 3: Euclidean distance with all vars\n",
    "# Group 4: Random other distance with only numeric vars\n",
    "# Group 5: Random other distance with all vars\n",
    "\n",
    "# Group 6: Euclidean distance with only numeric vars and user weights\n",
    "# Group 7: Euclidean distance with all vars and user weights\n",
    "# Group 8: Random other distance with only numeric vars and user weights\n",
    "# Group 9: Random other distance with all vars and user weights\n",
    "\n",
    "groups = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "distances = [manhattan, canberra, chebyshev, minkowski6, minkowski12]\n",
    "\n",
    "# Randomly assign user to a group and a distance metric\n",
    "user_group = random.choice(groups)\n",
    "if user_group in [4, 5, 8, 9]:\n",
    "  user_distance = random.choice(distances)\n",
    "else:\n",
    "  user_distance = euclidean\n",
    "\n",
    "# Allow user to provide weights if in certain groups\n",
    "if user_group in [6, 7, 8, 9]:\n",
    "  danceability_weight = input('On a scale of 1 to 5, how important is it to you that a song be \\\"danceable\\\"? (1 = not important, 5 = very important)')\n",
    "  valence_weight = input('On a scale of 1 to 5, how important is it to you that a song be happy/upbeat? (1 = not important, 5 = very important)')\n",
    "  acousticness_weight = input('On a scale of 1 to 5, how important is it to you that a song be acoustic? (1 = not important, 5 = very important)')\n",
    "  liveness_weight = input('On a scale of 1 to 5, how important is it to you that a song be live? (1 = not important, 5 = very important)')\n",
    "  energy_weight = input('On a scale of 1 to 5, how important is it to you that a song be energetic? (1 = not important, 5 = very important)')\n",
    "  user_weights = [1, 1, 1, int(danceability_weight), int(energy_weight), 1, int(acousticness_weight), 1, int(liveness_weight), 1, int(valence_weight)]\n",
    "else:\n",
    "  user_weights = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# If user is in group 1, no need to run K-Means\n",
    "if user_group == 1:\n",
    "  create_playlist(group1(recc_ids, saved_ids))\n",
    "\n",
    "print(user_group, user_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice! There are 2294 outliers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your playlist has been created!'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the entire K-Means process\n",
    "new_scaled, new_recc_clusters, new_nto_recc, new_ncombined, all_vars, metric = kmeans_process(euclidean, 7, user_weights, combined_df, recc_df, saved_df)\n",
    "\n",
    "# Make playlist and add it to user's library\n",
    "create_playlist(other_groups(new_nto_recc, new_ncombined, saved_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  os.remove('.cache') # Remove cache file storing user's profile information\n",
    "except:\n",
    "  print('No cache file to remove.')\n",
    "# Reset all variables\n",
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
