{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules, prep spotipy oauth\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyOAuth\n",
    "import cred\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.spatial import distance as dist\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from pyclustering.cluster.kmeans import kmeans as pykmeans\n",
    "from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n",
    "from pyclustering.utils.metric import distance_metric, type_metric\n",
    "\n",
    "\n",
    "scope = \"playlist-read-private, playlist-modify-public, user-read-private, user-top-read, user-library-read\"\n",
    "auth_manager = SpotifyOAuth(client_id=cred.client_id, client_secret=cred.client_secret, redirect_uri='http://127.0.0.1:8080', scope=scope)\n",
    "sp = spotipy.Spotify(auth_manager=auth_manager, requests_timeout=10, retries=5)\n",
    "\n",
    "user_id = sp.current_user()['id']\n",
    "user_country = sp.current_user()['country']\n",
    "username = sp.current_user()['display_name']\n",
    "if sp.current_user()['explicit_content']['filter_enabled'] or sp.current_user()['explicit_content']['filter_locked']:\n",
    "  user_explicit = False\n",
    "else:\n",
    "  user_explicit = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def get_user_top_tracks():\n",
    "\n",
    "  long_term_tracks = sp.current_user_top_tracks(time_range='long_term', limit=30)\n",
    "  medium_term_tracks = sp.current_user_top_tracks(time_range='medium_term', limit=30)\n",
    "  short_term_tracks = sp.current_user_top_tracks(time_range='short_term', limit=30)\n",
    "\n",
    "  top_track_ids = []\n",
    "  for ltrack in long_term_tracks['items']:\n",
    "    top_track_ids.append(ltrack['uri'])\n",
    "\n",
    "  for mtrack in medium_term_tracks['items']:\n",
    "    top_track_ids.append(mtrack['uri'])\n",
    "\n",
    "  for strack in short_term_tracks['items']:\n",
    "    top_track_ids.append(strack['uri'])\n",
    "\n",
    "  return list(set(top_track_ids))\n",
    "\n",
    "def get_user_playlist_ids():\n",
    "  '''\n",
    "  Collects a list of user playlist dictionaries and the Spotify ID for each of them.\n",
    "  '''\n",
    "  playlists_lst =[]\n",
    "  ids = []\n",
    "  offset = 0\n",
    "  while True:\n",
    "      playlists = sp.current_user_playlists(offset=offset)\n",
    "      if len(playlists['items']) == 0:\n",
    "          break\n",
    "      for playlist in playlists['items']:\n",
    "          playlists_lst.append(playlist)\n",
    "      offset = offset + len(playlists['items'])\n",
    "      time.sleep(0.0001) \n",
    "  \n",
    "  for playlist in playlists_lst:\n",
    "      ids.append(playlist['id'])\n",
    "  return ids, playlists_lst\n",
    "\n",
    "def get_user_top_tracks():\n",
    "\n",
    "  long_term_tracks = sp.current_user_top_tracks(time_range='long_term', limit=30)\n",
    "  medium_term_tracks = sp.current_user_top_tracks(time_range='medium_term', limit=30)\n",
    "  short_term_tracks = sp.current_user_top_tracks(time_range='short_term', limit=30)\n",
    "\n",
    "  top_track_ids = []\n",
    "  for ltrack in long_term_tracks['items']:\n",
    "    top_track_ids.append(ltrack['uri'])\n",
    "\n",
    "  for mtrack in medium_term_tracks['items']:\n",
    "    top_track_ids.append(mtrack['uri'])\n",
    "\n",
    "  for strack in short_term_tracks['items']:\n",
    "    top_track_ids.append(strack['uri'])\n",
    "\n",
    "  return list(set(top_track_ids))\n",
    "\n",
    "def get_saved_tracks():\n",
    "  '''\n",
    "  Gets a user's \"liked\" tracks\n",
    "  '''\n",
    "  ids = []\n",
    "  print('I\\'m starting to look at the user\\'s saved tracks!!')\n",
    "  offset = 0\n",
    "  t1 = time.time()\n",
    "  while True:\n",
    "      track_ids = sp.current_user_saved_tracks(offset=offset)\n",
    "      if len(track_ids['items']) == 0:\n",
    "          break\n",
    "      for track in track_ids['items']:\n",
    "          if track['track'] == None:\n",
    "              continue\n",
    "          else:\n",
    "              ids.append(track['track']['id'])\n",
    "      offset = offset + len(track_ids['items'])\n",
    "      time.sleep(0.0001)\n",
    "  t2 = time.time()\n",
    "  print(f'Hmmm... getting the liked tracks took {t2-t1} seconds!\\n')\n",
    "  return list(set(ids))\n",
    "\n",
    "def get_playlist_names(playlists):\n",
    "  '''\n",
    "  Returns a list of a user's playlist titles when given a list of playlist ids\n",
    "  '''\n",
    "  names = []\n",
    "  for playlist in playlists:\n",
    "      name = sp.playlist(playlist)['name']\n",
    "      names.append(name)\n",
    "  return names\n",
    "\n",
    "def get_song_ids_from_playlists(user, playlist_urls):\n",
    "  '''\n",
    "  Gets song ids from each of the songs in given playlist ids\n",
    "  '''\n",
    "  ids = []\n",
    "  t1 = time.time()\n",
    "  for i in range(len(playlist_urls)):\n",
    "      offset = 0\n",
    "      playlist_name = get_playlist_names([playlist_urls[i]])\n",
    "      print(f'I\\'m grabbing saved songs from playlist number {i+1} out of {len(playlist_urls)}: {playlist_name[0]}')\n",
    "      while True:\n",
    "          track_ids = sp.user_playlist_tracks(user=user, playlist_id=playlist_urls[i], offset=offset, fields ='items.track.id')\n",
    "          #print(track_ids)\n",
    "          #print(len(track_ids['items']))\n",
    "          if len(track_ids['items']) == 0:\n",
    "              break\n",
    "          for track in track_ids['items']:\n",
    "              if track['track'] == None:\n",
    "                  continue\n",
    "              else:\n",
    "                  ids.append(track['track']['id'])\n",
    "          offset = offset + len(track_ids['items'])\n",
    "          time.sleep(0.0001)\n",
    "  t2 = time.time()\n",
    "  print(f'Getting song ids from all those playlists took {round(t2-t1, 2)} seconds!\\n')\n",
    "  return list(set(ids))\n",
    "\n",
    "def get_recc_ids(list_seed_tracks, country):\n",
    "  '''\n",
    "  Gets ids for # recommended songs for each song in the seed tracks list\n",
    "  '''\n",
    "  print('Starting to collect recommendation ids.')\n",
    "  if len(list_seed_tracks) > 150:\n",
    "    print(f'Wow! I have {len(list_seed_tracks)*20} to make. This may take a while.\\n')\n",
    "\n",
    "  recc_ids = []\n",
    "  #raw_recs = []\n",
    "  t1 = time.time()\n",
    "  for seed in list_seed_tracks:\n",
    "    seed_to_use = []\n",
    "    seed_to_use.append(seed)\n",
    "    recs = sp.recommendations(seed_tracks=seed_to_use, limit = 30, country=country)\n",
    "    #raw_recs.append(recs)\n",
    "    #print(recs)\n",
    "    for i in range(len(recs['tracks'])):\n",
    "      track_id = recs['tracks'][i]['id']\n",
    "      if track_id not in recc_ids:\n",
    "        recc_ids.append(track_id)\n",
    "    #print(len(recc_ids))\n",
    "  set_ids = set(recc_ids) # extra check to make sure there are no duplicates\n",
    "  t2 = time.time()\n",
    "  print(f'Making and saving all of those recommendations took {round(t2-t1, 2)} seconds.\\n')\n",
    "  return list(set_ids)\n",
    "\n",
    "def create_playlist(tracks):\n",
    "  sp.user_playlist_create(user_id, 'your recommended songs', description='yay new songs!')\n",
    "  user_playlists, y = get_user_playlist_ids()\n",
    "  sp.user_playlist_add_tracks(user_id, user_playlists[0], tracks)\n",
    "  return 'Your playlist has been created!'\n",
    "\n",
    "def create_df(track_ids, in_lib):\n",
    "  print(f'{len(track_ids)} observations to make!')\n",
    "  data = []\n",
    "\n",
    "  for i in range(len(track_ids)):\n",
    "    # Get raw data for track\n",
    "    try:\n",
    "      track = sp.track(track_ids[i])\n",
    "      features = sp.audio_features(track_ids[i])\n",
    "      analysis = sp.audio_analysis(track_ids[i])\n",
    "      artist_uri = track['album']['artists'][0]['uri']\n",
    "      artist = sp.artist(artist_uri)\n",
    "      decade_prep = track['album']['release_date'][0:3]\n",
    "      decade = int(decade_prep + '0')\n",
    "      if int(track['album']['release_date'][3]) >= 0 and int(track['album']['release_date'][3]) < 5:\n",
    "        half_decade = int(track['album']['release_date'][0:3] + '0')\n",
    "      else:\n",
    "        half_decade = int(track['album']['release_date'][0:3] + '5')\n",
    "    except:\n",
    "      print(f'skipped one ({in_lib})!')\n",
    "      continue\n",
    "\n",
    "    \n",
    "    \n",
    "    # Extract relevant data\n",
    "    observation = [\n",
    "      track['uri'], \n",
    "      track['name'],\n",
    "      in_lib,\n",
    "      artist['followers']['total'],\n",
    "      artist['genres'],\n",
    "      artist['popularity'],\n",
    "      track['explicit'],\n",
    "      track['album']['release_date'][0:4], \n",
    "      int(track['album']['release_date'][0:4]),\n",
    "      decade,\n",
    "      half_decade,\n",
    "      len(track['artists']),\n",
    "      round(track['duration_ms']/60000, 4),\n",
    "      track['popularity'],\n",
    "      features[0]['danceability'],\n",
    "      features[0]['energy'],\n",
    "      features[0]['key'],\n",
    "      analysis['track']['key_confidence'],\n",
    "      features[0]['loudness'],\n",
    "      features[0]['mode'],\n",
    "      analysis['track']['mode_confidence'],\n",
    "      features[0]['speechiness'],\n",
    "      features[0]['acousticness'],\n",
    "      features[0]['instrumentalness'],\n",
    "      features[0]['liveness'],\n",
    "      features[0]['valence'],\n",
    "      features[0]['tempo'],\n",
    "      analysis['track']['tempo_confidence'],\n",
    "      features[0]['time_signature'],\n",
    "      analysis['track']['time_signature_confidence'],\n",
    "      analysis['track']['num_samples'],\n",
    "      len(analysis['bars']),\n",
    "      len(analysis['beats']),\n",
    "      len(analysis['sections']),\n",
    "      len(analysis['segments']), # for each segment, there is a list of pitches and timbre!\n",
    "      len(analysis['tatums'])\n",
    "    ]\n",
    "\n",
    "    # Add observation to total dataset\n",
    "    data.append(observation)\n",
    "    time.sleep(0.00000001)\n",
    "    \n",
    "  # Create final data frame with proper column names\n",
    "  df = pd.DataFrame(data, columns=[\n",
    "    'uri', 'track_name', 'in_library', 'artist_followers', 'genre', 'artist_popularity', 'explicit', 'release_date', \n",
    "    'year', 'decade', 'half_decade', 'nartists', 'duration_m', 'track_popularity', 'danceability', 'energy',\n",
    "    'key', 'key_conf', 'loudness', 'mode', 'mode_conf', 'speechiness', 'acousticness', 'instrumentalness',\n",
    "    'liveness', 'valence', 'tempo', 'tempo_conf', 'time_sig', 'time_sig_conf', 'nsamples', 'nbars',\n",
    "    'nbeats', 'nsections', 'nsegments', 'ntatums'\n",
    "  ])\n",
    "\n",
    "  return df\n",
    "\n",
    "def df_manage(reccs, saved):\n",
    "  recc_df = create_df(reccs, 0) # df of recommendations\n",
    "  saved_df = create_df(saved, 1) # df of songs in library\n",
    "  combined_df = pd.concat([recc_df, saved_df], ignore_index = True) # the previous two combined\n",
    "  return recc_df, saved_df, combined_df\n",
    "\n",
    "def count_predict(kmeans, user_weights, group, clusters, recc_df, saved_df, num_feat, ord_feat, nom_feat):\n",
    "  \n",
    "  scaled_reccs, x = scaling(group, recc_df, user_weights, num_feat, ord_feat, nom_feat)\n",
    "  scaled_saved, x = scaling(group, saved_df, user_weights, num_feat, ord_feat, nom_feat)\n",
    "\n",
    "  # Predict clusters for recommendations\n",
    "  recc_predictions = kmeans.predict(scaled_reccs)\n",
    "  \n",
    "  # Predict clusters for saved tracks\n",
    "  saved_predictions = kmeans.predict(scaled_saved)\n",
    "  # print(saved_predictions)\n",
    "\n",
    "  # Set counts for all recc clusters to 0 (ensures that all clusters are present)\n",
    "  initial_recc_counts = {}\n",
    "  for i in range(0, clusters):  \n",
    "    initial_recc_counts[i] = 0\n",
    "\n",
    "  # Create a counter and add in predicted cluster counts for recommendations\n",
    "  cluster_recc_counts = Counter(initial_recc_counts)\n",
    "  cluster_recc_counts.update(recc_predictions)\n",
    "\n",
    "  # Set counts for all saved clusters to 0 (ensures that all clusters are present)\n",
    "  initial_saved_counts = {}\n",
    "  for i in range(0, clusters):  \n",
    "    initial_saved_counts[i] = 0\n",
    "\n",
    "  # Create a counter and add in predicted cluster counts for saved tracks\n",
    "  cluster_saved_counts = Counter(initial_saved_counts)\n",
    "  cluster_saved_counts.update(saved_predictions)\n",
    "\n",
    "  # Create new dict with percentages of songs in library that are in each of the clusters\n",
    "  cluster_prop = {}\n",
    "  for item in cluster_saved_counts:\n",
    "    cluster_prop[item] = cluster_saved_counts[item] / saved_df.shape[0]\n",
    "\n",
    "  n_to_recc = {}\n",
    "  for item in cluster_prop:\n",
    "    n_to_recc[item] = round(cluster_prop[item]*30, 7)\n",
    "  \n",
    "  return recc_predictions, cluster_recc_counts, cluster_saved_counts, cluster_prop, n_to_recc\n",
    "\n",
    "def add_cluster(recc_data, saved_data, clusters):\n",
    "  recc_with_clusters = recc_data.assign(cluster = clusters)\n",
    "  saved_with_clusters = saved_data.assign(cluster = max(clusters) + 1)\n",
    "  new_combined = pd.concat([recc_with_clusters, saved_with_clusters], ignore_index = True)\n",
    "  return recc_with_clusters, new_combined\n",
    "\n",
    "def silhouette_plot(dist_metric, scaled_og_data):\n",
    "  range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "  for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-1, 1]\n",
    "    ax1.set_xlim([-1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(scaled_og_data) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    initial_centers = kmeans_plusplus_initializer(data = scaled_og_data, amount_centers = n_clusters).initialize()\n",
    "    clusterer = pykmeans(scaled_og_data, initial_centers, metric = dist_metric)\n",
    "    clusterer.process()\n",
    "    #clusters = kmeans_instance.get_clusters()\n",
    "    #final_centers = kmeans_instance.get_centers()\n",
    "    # type(final_centers[0])\n",
    "    cluster_labels = clusterer.predict(scaled_og_data)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(scaled_og_data, cluster_labels)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(scaled_og_data, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "      # Aggregate the silhouette scores for samples belonging to\n",
    "      # cluster i, and sort them\n",
    "      ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "      ith_cluster_silhouette_values.sort()\n",
    "\n",
    "      size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "      y_upper = y_lower + size_cluster_i\n",
    "\n",
    "      color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "      ax1.fill_betweenx(\n",
    "        np.arange(y_lower, y_upper),\n",
    "        0,\n",
    "        ith_cluster_silhouette_values,\n",
    "        facecolor=color,\n",
    "        edgecolor=color,\n",
    "        alpha=0.7,\n",
    "      )\n",
    "\n",
    "      # Label the silhouette plots with their cluster numbers at the middle\n",
    "      ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "      # Compute the new y_lower for next plot\n",
    "      y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-1, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "      scaled_og_data[:, 0], scaled_og_data[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = np.array(clusterer.get_centers())\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(\n",
    "      centers[:, 0],\n",
    "      centers[:, 1],\n",
    "      marker=\"o\",\n",
    "      c=\"white\",\n",
    "      alpha=1,\n",
    "      s=200,\n",
    "      edgecolor=\"k\",\n",
    "    )\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "      ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 3rd feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 4th feature\")\n",
    "\n",
    "    plt.suptitle(\n",
    "      \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
    "      % n_clusters,\n",
    "      fontsize=14,\n",
    "      fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "def diagnostics(algo, metric, scaled_data, fit_data, vars, pairplot, silhouette, elbow):\n",
    "  \n",
    "  if pairplot:\n",
    "    if len(vars) == 2:\n",
    "      sns.scatterplot(data = fit_data, x = vars[0], y = vars[1])\n",
    "      plt.show()\n",
    "\n",
    "      sns.scatterplot(data = fit_data, x = vars[0], y = vars[1], hue = 'cluster', palette = 'Spectral')\n",
    "      plt.show()\n",
    "    else:\n",
    "      sns.pairplot(data = fit_data, vars = vars, hue = 'cluster', palette = 'Spectral')\n",
    "      plt.show()\n",
    "  \n",
    "  if silhouette:\n",
    "    silhouette_plot(metric, scaled_data)\n",
    "\n",
    "  if elbow and algo == 'sklearn':\n",
    "    distortions = []\n",
    "    for k in range(2, 15):\n",
    "      kmeans = KMeans(n_clusters=k, random_state=10, init='k-means++')\n",
    "      kmeans.fit(scaled_data)\n",
    "      distortions.append(kmeans.inertia_)\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    plt.plot(range(2, 15), distortions)\n",
    "    plt.grid(True)\n",
    "    plt.title('Elbow curve')\n",
    "  \n",
    "def detect_outlier(scaled_data, combined_df):\n",
    "  from sklearn.ensemble import IsolationForest\n",
    "  iso = IsolationForest(contamination=0.1)\n",
    "  yhat_iso = iso.fit_predict(scaled_data)\n",
    "  obs_iso = np.where(yhat_iso == -1)[0]\n",
    "\n",
    "  from sklearn.neighbors import LocalOutlierFactor\n",
    "  lof = LocalOutlierFactor()\n",
    "  yhat_lof = lof.fit_predict(scaled_data)\n",
    "  obs_lof = np.where(yhat_lof == -1)[0]\n",
    "\n",
    "  from sklearn.svm import OneClassSVM\n",
    "  ee = OneClassSVM(nu=0.01)\n",
    "  yhat_ee = ee.fit_predict(scaled_data)\n",
    "  obs_ocs = np.where(yhat_ee == -1)[0]\n",
    "\n",
    "  common_outliers = sorted(set(obs_iso).union(set(obs_lof), set(obs_ocs)))\n",
    "  # print(common_outliers)\n",
    "  recc_outliers = []\n",
    "  for index in common_outliers:\n",
    "    if combined_df.iloc[index]['in_library'] == 0:\n",
    "      recc_outliers.append(index)\n",
    "\n",
    "  print(f'Nice! There are {len(recc_outliers)} outliers.')\n",
    "  return recc_outliers\n",
    "\n",
    "def manhattan(object1, object2):\n",
    "  return dist.cityblock(object1, object2)\n",
    "\n",
    "def chebyshev(object1, object2):\n",
    "  return dist.chebyshev(object1, object2)\n",
    "\n",
    "def minkowski6(object1, object2):\n",
    "  if len(object1.shape) > 1 or len(object2.shape) > 1:\n",
    "    return np.power(np.sum(np.power(object1 - object2, 6), axis=1), 1/6)\n",
    "  else:\n",
    "    return np.power(np.sum(np.power(object1 - object2, 6)), 1 / 6)\n",
    "\n",
    "def minkowski12(object1, object2):\n",
    "  if len(object1.shape) > 1 or len(object2.shape) > 1:\n",
    "    return np.power(np.sum(np.power(object1 - object2, 12), axis=1), 1/12)\n",
    "  else:\n",
    "    return np.power(np.sum(np.power(object1 - object2, 12)), 1 / 12)\n",
    "\n",
    "def canberra(object1, object2):\n",
    "  return dist.canberra(object1, object2)\n",
    "\n",
    "def euclidean(object1, object2):\n",
    "  return dist.euclidean(object1, object2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all playlist ids, song ids, and recc ids\n",
    "\n",
    "playlist_ids, raw_playlists = get_user_playlist_ids()\n",
    "print(f'Cool! You have {len(playlist_ids)} playlists in your library!')\n",
    "\n",
    "playlist_names = get_playlist_names(playlist_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract various playlists for testing\n",
    "list_nc = ['https://open.spotify.com/playlist/7bK69JWbNKgHDqgpvj6RIJ?si=39ebca3ad1bf4265']\n",
    "list_class = ['https://open.spotify.com/playlist/0OLNCNPqHhbyEw0QFFTPOF?si=ee0540375d824c4a']\n",
    "list_pop = ['https://open.spotify.com/playlist/16BmzvIRzDl24kP1hh1UAE?si=2b4b3fbd72f44bb8']\n",
    "list_rock = ['https://open.spotify.com/playlist/4udhwyfNNJ58pdbZWJtMCa?si=37f8d6b49a144a91']\n",
    "list_div = ['https://open.spotify.com/playlist/2jzycRQnSyps1rhDxfVa6Q?si=1438231452824c81']\n",
    "\n",
    "# Get the song ids for all tracks in the test playlists\n",
    "tracks_nc = get_song_ids_from_playlists(user_id, list_nc)\n",
    "tracks_class = get_song_ids_from_playlists(user_id, list_class)\n",
    "tracks_pop = get_song_ids_from_playlists(user_id, list_pop)\n",
    "tracks_rock = get_song_ids_from_playlists(user_id, list_rock)\n",
    "tracks_div = get_song_ids_from_playlists(user_id, list_div)\n",
    "# top_tracks = get_user_top_tracks()\n",
    "print(f'Whoa...you have {len(song_ids)} songs in your library (using the playlists provided).')\n",
    "\n",
    "\n",
    "recc_nc = get_recc_ids(tracks_nc, user_country)\n",
    "recc_class = get_recc_ids(tracks_class, user_country)\n",
    "recc_pop = get_recc_ids(tracks_pop, user_country)\n",
    "recc_rock = get_recc_ids(tracks_rock, user_country)\n",
    "recc_diverse_50 = get_recc_ids(tracks_div, user_country)\n",
    "\n",
    "\n",
    "def remove_explicit(recc_ids):\n",
    "  for recc in recc_ids:\n",
    "    if sp.track(recc)['explicit']:\n",
    "      recc_ids.remove(recc)\n",
    "\n",
    "if not user_explicit:\n",
    "  remove_explicit(recc_nc)\n",
    "  remove_explicit(recc_class)\n",
    "  remove_explicit(recc_pop)\n",
    "  remove_explicit(recc_rock)\n",
    "  remove_explicit(recc_diverse_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build data frames\n",
    "# 30 RECOMMENDATIONS PER SONG IN LIBRARY\n",
    "\n",
    "recc_df_nc, saved_df_nc, combined_df_nc = df_manage(recc_nc, tracks_nc)\n",
    "recc_df_class, saved_df_class, combined_df_class = df_manage(recc_class, tracks_class)\n",
    "recc_df_pop, saved_df_pop, combined_df_pop = df_manage(recc_pop, tracks_pop)\n",
    "recc_df_rock, saved_df_rock, combined_df_rock = df_manage(recc_rock, tracks_rock)\n",
    "recc_df_div, saved_df_div, combined_df_div = df_manage(recc_diverse, tracks_div)\n",
    "#60ish for 20 reccs\n",
    "#72ish for 30 reccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale all data, fit new kmeans\n",
    "\n",
    "# Group 1: Random 30 basic reccs\n",
    "\n",
    "# Group 2: Euclidean distance with only numeric vars\n",
    "# Group 3: Euclidean distance with all vars\n",
    "# Group 4: Random other distance with only numeric vars\n",
    "# Group 5: Random other distance with all vars\n",
    "\n",
    "# Group 6: Euclidean distance with only numeric vars and user weights\n",
    "# Group 7: Euclidean distance with all vars and user weights\n",
    "# Group 8: Random other distance with only numeric vars and user weights\n",
    "# Group 9: Random other distance with all vars and user weights\n",
    "\n",
    "groups = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "distances = [manhattan, canberra, chebyshev, minkowski6, minkowski12]\n",
    "\n",
    "user_group = random.choice(groups)\n",
    "if user_group in [4, 5, 8, 9]:\n",
    "  user_distance = random.choice(distances)\n",
    "else:\n",
    "  user_distance = euclidean\n",
    "\n",
    "if user_group in [6, 7, 8, 9]:\n",
    "  danceability_weight = input('On a scale of 1 to 5, how important is it to you that a song be \\\"danceable\\\"? (1 = not important, 5 = very important)')\n",
    "  valence_weight = input('On a scale of 1 to 5, how important is it to you that a song be happy/upbeat? (1 = not important, 5 = very important)')\n",
    "  acousticness_weight = input('On a scale of 1 to 5, how important is it to you that a song be acoustic? (1 = not important, 5 = very important)')\n",
    "  liveness_weight = input('On a scale of 1 to 5, how important is it to you that a song be live? (1 = not important, 5 = very important)')\n",
    "  energy_weight = input('On a scale of 1 to 5, how important is it to you that a song be energetic? (1 = not important, 5 = very important)')\n",
    "  user_weights = [1, 1, 1, int(danceability_weight), int(energy_weight), 1, int(acousticness_weight), 1, int(liveness_weight), 1, int(valence_weight)]\n",
    "\n",
    "def scaling(group, combined_df, weights, numeric_features, ordinal_features, nominal_features):\n",
    "\n",
    "  num_transformer = StandardScaler()\n",
    "  ordnom_transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"ord\", OrdinalEncoder(), ordinal_features),\n",
    "            ('nom', OneHotEncoder(), nominal_features)\n",
    "        ]\n",
    "    )\n",
    "  \n",
    "  if group in [2, 4]:\n",
    "    scaled_data = num_transformer.fit_transform(combined_df[numeric_features])\n",
    "    all_vars = numeric_features\n",
    "  elif group in [6, 8]:\n",
    "    scaled_data = num_transformer.fit_transform(combined_df[numeric_features])\n",
    "    for i in range(len(weights)):\n",
    "      scaled_data[:, i] *= weights[i]**(1/2)\n",
    "    all_vars = numeric_features\n",
    "  \n",
    "  elif group in [3, 5]:\n",
    "    scaled_num = num_transformer.fit_transform(combined_df[numeric_features])\n",
    "    scaled_ordnom = ordnom_transformer.fit_transform(combined_df[ordinal_features + nominal_features])\n",
    "    scaled_data = np.concatenate((scaled_num, scaled_ordnom), axis = 1)\n",
    "    all_vars = numeric_features + ordinal_features + nominal_features\n",
    "  elif group in [7, 9]:\n",
    "    scaled_num = num_transformer.fit_transform(combined_df[numeric_features])\n",
    "    scaled_ordnom = ordnom_transformer.fit_transform(combined_df[ordinal_features + nominal_features])\n",
    "    for i in range(len(weights)):\n",
    "      scaled_num[:, i] *= weights[i]**(1/2)\n",
    "    scaled_data = np.concatenate((scaled_num, scaled_ordnom), axis = 1)\n",
    "    all_vars = numeric_features + ordinal_features + nominal_features\n",
    "  \n",
    "  return scaled_data, all_vars\n",
    "\n",
    "def kmeans_process(distance, group, user_weights, combined_df, recc_df, saved_df):\n",
    "\n",
    "  if distance in ['chebyshev', 'minkowski_6', 'euclidean', 'minkowski_12']:\n",
    "    clusters = 3\n",
    "  else:\n",
    "    clusters = 2\n",
    "\n",
    "  if group in [2, 4, 6, 8]:\n",
    "    numeric = True\n",
    "  else:\n",
    "    numeric = False\n",
    "\n",
    "  print(distance, group, numeric, user_weights)\n",
    "\n",
    "  numeric_features = ['artist_popularity',\n",
    "    'duration_m', 'track_popularity', 'danceability', 'energy',\n",
    "    'loudness', 'acousticness', 'instrumentalness',\n",
    "    'liveness', 'tempo', 'valence']\n",
    "  ordinal_features = ['half_decade', 'key', 'time_sig']\n",
    "  if user_explicit:\n",
    "    nominal_features = ['explicit', 'mode']\n",
    "  else:\n",
    "    nominal_features = ['mode']\n",
    "\n",
    "  scaled_data, all_vars = scaling(group, combined_df, user_weights, numeric_features, ordinal_features, nominal_features)\n",
    "  \n",
    "  metric = distance_metric(type_metric.USER_DEFINED, func=distance)\n",
    "\n",
    "  initial_centers = kmeans_plusplus_initializer(data = scaled_data, amount_centers = clusters).initialize()\n",
    "  kmeans = pykmeans(scaled_data, initial_centers, metric = metric)\n",
    "  kmeans.process()\n",
    "\n",
    "  # recc_clusters, recc_count, saved_count, saved_prop, nto_recc = count_predict(kmeans, numeric, recc_df, saved_df, all_vars, numeric_features, ordinal_features, nominal_features)\n",
    "  # recc_with_cluster, combined_with_cluster = add_cluster(recc_df, saved_df, recc_clusters)\n",
    "\n",
    "  new_recc_df = recc_df.drop(detect_outlier(scaled_data, combined_df), axis = 0, inplace = False)\n",
    "  new_combined_df = pd.concat([new_recc_df, saved_df], ignore_index = True)\n",
    "  new_scaled_data, _ = scaling(group, new_combined_df, user_weights, numeric_features, ordinal_features, nominal_features)\n",
    "\n",
    "  new_initial_centers = kmeans_plusplus_initializer(data = new_scaled_data, amount_centers = clusters).initialize()\n",
    "  new_kmeans = pykmeans(new_scaled_data, new_initial_centers, metric = metric)\n",
    "  new_kmeans.process()\n",
    "\n",
    "  new_recc_clusters, new_recc_count, new_saved_count, new_saved_prop, new_nto_recc = count_predict(new_kmeans, user_weights, group, clusters, new_recc_df, saved_df, numeric_features, ordinal_features, nominal_features)\n",
    "\n",
    "  new_recc_df_clusters, new_ncombined = add_cluster(new_recc_df, saved_df, new_recc_clusters)\n",
    "\n",
    "  return new_scaled_data, new_recc_clusters, new_nto_recc, new_ncombined, all_vars, metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function euclidean at 0x12204c1f0> 6 True [1, 1, 1, 3, 4, 1, 1, 1, 2, 1, 2]\n",
      "Nice! There are 140 outliers.\n"
     ]
    }
   ],
   "source": [
    "# Group 1: Random 30 basic reccs\n",
    "\n",
    "# Group 2: Euclidean distance with only numeric vars\n",
    "# Group 3: Euclidean distance with all vars\n",
    "# Group 4: Random other distance with only numeric vars\n",
    "# Group 5: Random other distance with all vars\n",
    "\n",
    "# Group 6: Euclidean distance with only numeric vars and user weights\n",
    "# Group 7: Euclidean distance with all vars and user weights\n",
    "# Group 8: Random other distance with only numeric vars and user weights\n",
    "# Group 9: Random other distance with all vars and user weights\n",
    "\n",
    "# manhattan, canberra, chebyshev, minkowski6, minkowski12\n",
    "\n",
    "new_scaled_rock, new_recc_clusters_rock, new_nto_recc_rock, new_ncombined_rock, all_vars, metric = kmeans_process(user_distance, user_group, user_weights, combined_df_rock, recc_df_rock, saved_df_rock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommendations\n",
    "\n",
    "def group1(og_reccs, in_lib_tracks):\n",
    "\n",
    "  random.shuffle(og_reccs)\n",
    "\n",
    "  if len(og_reccs) <= 30:\n",
    "    group1 = og_reccs\n",
    "  else:\n",
    "    group1 = []\n",
    "    for i in range(len(og_reccs)):\n",
    "      if og_reccs[i] not in in_lib_tracks:\n",
    "        group1.append(og_reccs[i])\n",
    "      if len(group1) == 30:\n",
    "        break\n",
    "\n",
    "  return group1\n",
    "\n",
    "def other_groups(to_recc_counts, clustered_data, in_lib_tracks):\n",
    "  group2 = []\n",
    "  \n",
    "  for cluster in to_recc_counts.keys():\n",
    "    one_cluster = clustered_data[clustered_data.cluster.eq(cluster)] # Select one cluster\n",
    "    uris = list(one_cluster['uri']) # Select only uris\n",
    "    reccs = random.sample(uris, k = round(to_recc_counts[cluster])) # take a random sample of the correct size\n",
    "    for i in range(len(reccs)):\n",
    "      group2.append(reccs[i]) # Append uris to group2\n",
    "\n",
    "  # Remove any tracks already in library\n",
    "  for i in range(len(group2)):\n",
    "    if group2[i] in in_lib_tracks:\n",
    "      group1.remove(group2[i])\n",
    "  return group2\n",
    "\n",
    "# if user_group == 1:\n",
    "#   create_playlist(group1(recc_nc, tracks_nc))\n",
    "# else:\n",
    "#   create_playlist(other_groups(new_nto_recc, new_ncombined, tracks_nc))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
